{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = 'distilbert/distilgpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2744864"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules import Decoder\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "gpt = Decoder(\n",
    "    num_layers = 2, \n",
    "    context = 256, \n",
    "    attn_heads = 4, \n",
    "    hidden_dim = 256, \n",
    "    mlp_hidden = 512, \n",
    "    reduced_dim = 32\n",
    ").to(device)\n",
    "\n",
    "sum([p.numel() for p in gpt.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def __init__(self, path, name, max_length, split = None, T = 32, **kwargs):\n",
    "        self.T = T\n",
    "        \n",
    "        dset = load_dataset(path, name, split=split, **kwargs)\n",
    "        self.len = dset.info.splits[split].num_examples\n",
    "        self.dset = iter(dset)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            next(self.dset)['text'],\n",
    "            padding = 'max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "            )['input_ids']\n",
    "        \n",
    "        if id + self.T >= self.len:\n",
    "            id = self.len - self.T - 1\n",
    "        \n",
    "        buf = tokenized[:self.T + 1]\n",
    "        inputs = torch.tensor(buf[:-1])\n",
    "        target = torch.tensor(buf[1:])\n",
    "        return inputs, target\n",
    "    \n",
    "train_set = TextGeneration(\n",
    "    'wikitext', 'wikitext-2-raw-v1',\n",
    "    max_length = gpt.config.context,\n",
    "    split='train', streaming = True,\n",
    "    T = 64\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e701c37d30d947389737b84562b4052b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Программы\\github\\transformers\\prototyping.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B/github/transformers/prototyping.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m logits \u001b[39m=\u001b[39m gpt(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B/github/transformers/prototyping.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m lossfn(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), target\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B/github/transformers/prototyping.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B/github/transformers/prototyping.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B/github/transformers/prototyping.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss_history\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\dlaptev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dlaptev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dlaptev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(gpt.parameters(), lr=3e-4)\n",
    "\n",
    "loss_history = []\n",
    "epochs = 1\n",
    "\n",
    "pbar = tqdm(range(epochs * len(train_loader)))\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, target = [v.to(device) for v in batch]\n",
    "        logits = gpt(inputs)\n",
    "        loss = lossfn(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        pbar.set_description(f'Epoch: {epoch}, loss: {loss.item():.5f}')\n",
    "        pbar.update(1)\n",
    "    \n",
    "plt.plot(loss_history)\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt, './gpt_wikitext.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TextGeneration(\n",
    "    'wikitext', 'wikitext-2-raw-v1',\n",
    "    max_length = gpt.config.context,\n",
    "    split='test', streaming = True,\n",
    "    T = 64\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5199,   347,  2852,   353,   318,   281,  3594,  2646,   837,  5581,\n",
      "           290, 21421,  8674,   764,   679,   550,   257,  8319,  2488,    12,\n",
      "            31, 20495,  2597,   319,   262,  5581,  2168,   383,  3941,   287,\n",
      "          4751,   764,   770,   373,  3940,   416,   257, 20495,  2597,   287,\n",
      "           262,   711,  2332,   684,  3194,   416, 11288, 37072,   837,   543,\n",
      "           373,  6157,   287,  5878,   379,   262,  8111,  3078, 15752,   764,\n",
      "           679,   550,   257,  8319]])\n",
      "tensor([  347,  2852,   353,   318,   281,  3594,  2646,   837,  5581,   290,\n",
      "        21421,  8674,   764,   679,   550,   257,  8319,  2488,    12,    31,\n",
      "        20495,  2597,   319,   262,  5581,  2168,   383,  3941,   287,  4751,\n",
      "          764,   770,   373,  3940,   416,   257, 20495,  2597,   287,   262,\n",
      "          711,  2332,   684,  3194,   416, 11288, 37072,   837,   543,   373,\n",
      "         6157,   287,  5878,   379,   262,  8111,  3078, 15752,   764,   679,\n",
      "          550,   257,  8319,  2597])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = test_set[25]\n",
    "inputs = inputs.unsqueeze(0)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Robert Boulter is an English film, television and theatre actor. He had a guest @-@ starring role on the television series The Bill in 2000. This was followed by a starring role in the play Herons written by Simon Stephens, which was performed in 2001 at the Royal Court Theatre. He had a guest\n",
      " of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated = gpt.generate(inputs.to(device), max_new_tokens=25)\n",
    "\n",
    "decoded = (\n",
    "    tokenizer.batch_decode(inputs, skip_special_tokens=True),\n",
    "    tokenizer.batch_decode(generated['answer'], skip_special_tokens=True)\n",
    ")\n",
    "\n",
    "print(decoded[0][0])\n",
    "print(decoded[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " had a wide long or the north @-@ lane of the state of the south of the west from Europe to the same\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello, ML&DS Community! What'\n",
    "\n",
    "print(tokenizer.batch_decode(gpt.generate(\n",
    "    tokenizer(text, return_tensors='pt')['input_ids'].to(device),\n",
    "    max_new_tokens = 25\n",
    ")['answer'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
